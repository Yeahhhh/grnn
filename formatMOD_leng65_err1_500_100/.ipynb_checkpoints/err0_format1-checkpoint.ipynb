{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3\n",
    "### Due Apr. 15th by end of day.\n",
    "### Name your notebook by firstname.lastname.hw3.ipynb and email it to zhang@csc.lsu.edu\n",
    "\n",
    "In this HW, we take a look at text processing using RNN based on LSTM. Given a review (hundred to thousand words long) about a movie, we want to classify the review as positive or negative (sentiment analysis). A model is given in this notebook that runs RNN, consisting of a single layer of LSTM cells, on the word sequence of the rewiews. Each review is transformed into a vector, which is the average of the LSTM vectors over the sequence. A softmax layer is then used to classify the reviews based on the LSTM vector representation. \n",
    "\n",
    "Your goal is to train the model and gain some idea on the computation challenge when training \"deep\" model for a large number of iterations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for python 2\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['test']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from lstm import load_data, prepare_data, batches_idx, lstm_layer, rmsprop\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model meta parameters\n",
    "\n",
    " - n_hidden: # of neurons in the LSTM layer\n",
    " - n_words: # of words in the dictionary. We use most frequent words. All other words are treated as unknown.\n",
    " - n_sample: max size (# of reviews) of the train dataset and the test dataset. (We may not be able to reach the max because if we want only reviews up to 50 words, there may be less than max such reviews in the whole data collection.)  \n",
    " - maxlen: max length in number of words for a review to be considered. Review longer than this will be ignored. This is also the # of recurrent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "n_words = 500\n",
    "n_sample = 1000\n",
    "maxlen = 50\n",
    "train, test, translate = load_data('./', n_words, n_sample, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 42)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/len20/a.raw.mod_para0_err0.format1')\n",
    "#df = df.iloc[np.random.permutation(len(df))]\n",
    "print(df.shape)\n",
    "\n",
    "ntrain=500\n",
    "ntest=100\n",
    "\n",
    "d = df[0:ntrain]\n",
    "train = (d.ix[:,:-1].values.tolist(), d.ix[:,-1].values.tolist())\n",
    "\n",
    "d = df[ntrain:ntrain+ntest]\n",
    "test = (d.ix[:,:-1].values.tolist(), d.ix[:,-1].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of pos in train: 260 out of 500\n",
      "num of pos in test: 56 out of 100\n"
     ]
    }
   ],
   "source": [
    "print('num of pos in train:', numpy.sum(train[1]), 'out of', len(train[1]))\n",
    "print('num of pos in test:', numpy.sum(test[1]), 'out of', len(test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden=40\n",
    "n_words=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(n_hidden, n_words):\n",
    "\n",
    "    x = T.matrix('x', dtype='int64')\n",
    "    mask = T.matrix('mask', dtype='float64')\n",
    "    y = T.vector('y', dtype='int64')\n",
    "\n",
    "    # embedding params\n",
    "    randn = numpy.random.rand(n_words, n_hidden)\n",
    "    Wemb = theano.shared(0.01 * randn)\n",
    "    \n",
    "    #lstm layer\n",
    "    lstm = lstm_layer(n_hidden)\n",
    "    \n",
    "    # classifier params\n",
    "    CU = theano.shared(0.01 * numpy.random.randn(n_hidden, 2))\n",
    "    Cb = theano.shared(numpy.zeros((2,)))\n",
    "    params = lstm.params + [Wemb, CU, Cb]\n",
    "    \n",
    "    \n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    emb = Wemb[x.flatten()].reshape([n_timesteps, n_samples, n_hidden])\n",
    "    proj = lstm.calc_lstm(emb, mask)\n",
    "    proj = (proj * mask[:, :, None]).sum(axis=0)\n",
    "    proj = proj / mask.sum(axis=0)[:, None]\n",
    "\n",
    "    pred = T.nnet.softmax(T.dot(proj, CU) + Cb)\n",
    "\n",
    "    f_pred = theano.function([x, mask], pred.argmax(axis=1), name='f_pred')\n",
    "\n",
    "    off = 1e-8\n",
    "    cost = -T.log(pred[T.arange(n_samples), y] + off).mean()\n",
    "\n",
    "    return x, mask, y, params, cost, f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x, mask, y, params, cost, f_pred) = build_model(n_hidden, n_words)\n",
    "grads = T.grad(cost, params)    \n",
    "\n",
    "lr = T.scalar(name='lr')\n",
    "f_grad_shared, f_update = rmsprop(lr, params, grads, x, mask, y, cost)\n",
    "\n",
    "lrate=0.1\n",
    "updates = [(p, p - lrate * g) for p, g in zip(params, grads)]\n",
    "f_sgd = theano.function([x, mask, y], cost, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.07942192651 10 2.07662935346 20 2.07582157487 30 2.0757033015 40 2.07569089596 50 2.07569113127 60 2.07569183392 70 2.07569205305 80 2.07569199388 90 2.07569181683 100 2.07569159364 110 2.07569135255 120 2.07569110416 130 2.07569085234 140 2.07569059847 150 2.07569034297 160 2.07569008593 170 2.07568982731 180 2.07568956702 190 2.07568930497 200 2.07568904104 210 2.07568877511 220 2.07568850708 230 2.07568823682 240 2.07568796421 250 2.07568768914 260 2.07568741148 270 2.0756871311 280 2.07568684788 290 2.07568656168 300 2.07568627239 310 2.07568597987 320 2.07568568398 330 2.07568538459 340 2.07568508155 350 2.07568477473 360 2.07568446398 370 2.07568414915 380 2.0756838301 390 2.07568350668 400 2.07568317872 410 2.07568284608 420 2.07568250858 430 2.07568216607 440 2.07568181837 450 2.07568146532 460 2.07568110674 470 2.07568074246 480 2.07568037228 "
     ]
    }
   ],
   "source": [
    "#nepoch = 200\n",
    "nepoch = 10000\n",
    "cc = numpy.zeros((nepoch,))\n",
    "\n",
    "sampling_iter = 50\n",
    "nepoch2 = int(nepoch / sampling_iter)\n",
    "cc2 = numpy.zeros((nepoch2,))\n",
    "cc2_idx = numpy.zeros((nepoch2,))\n",
    "\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(nepoch):\n",
    "    cost = 0\n",
    "    for train_index in batches_idx(len(train[0]), 200):\n",
    "        y = [train[1][t] for t in train_index]\n",
    "        x = [train[0][t] for t in train_index]\n",
    "        x, mask, y = prepare_data(x, y)\n",
    "        \n",
    "        cost += f_grad_shared(x, mask, y)\n",
    "        f_update(lrate)\n",
    "        #cost += f_sgd(x, mask, y)\n",
    "    \n",
    "    cc[i] = cost\n",
    "    if i % 10 == 0:\n",
    "        print(i, cost, end = ' ')\n",
    "        #print(i)\n",
    "\n",
    "    if i % sampling_iter == 0:\n",
    "        tx2, tmask2, ty2 = prepare_data(test[0], test[1])\n",
    "        py2 = f_pred(tx2, tmask2)\n",
    "        err_rate = 1.0*numpy.sum(numpy.abs(ty2 - py2))/len(test[0])\n",
    "        i2 = int(i / sampling_iter)\n",
    "        cc2[i2] = err_rate\n",
    "        cc2_idx[i2] = i\n",
    "        #print(i, err_rate)\n",
    "\n",
    "\n",
    "print('')\n",
    "print('run time:', (time.time()-t0)/60.0, \"minutes\")  #training time in min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(cc[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx = 2\n",
    "ny = 1\n",
    "fig = plt.figure(figsize = [6 * nx, 4 * ny]) # inch\n",
    "\n",
    "ax0 = fig.add_subplot(ny, nx, 1)\n",
    "plot(cc)\n",
    "plt.legend(['training score'], loc='lower left')\n",
    "ax0.set_xlim(0)\n",
    "ax0.set_ylim(0)\n",
    "ax0.set_ylabel('training score')\n",
    "ax0.set_xlabel('epoch')\n",
    "\n",
    "ax1 = fig.add_subplot(ny, nx, 2)\n",
    "plot(cc2_idx, cc2, color='g')\n",
    "plt.legend(['test error'], loc='lower left')\n",
    "ax1.set_xlim(0)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_ylabel('testing error')\n",
    "ax1.set_xlabel('epoch')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.min(cc))\n",
    "print(np.min(cc2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
